# Adaptation Strategies for Agentic AI: A Comprehensive Taxonomy

## 배경
### AI Agent
에이전트 인공지능(Agentic AI) 시스템은 환경을 인식(Perceive)하고, 추론(Reason)하며, 행동(Act)하고, 환경과의 상호작용을 통해 지속적으로 개선될 수 있는 자율적인 시스템을 의미합니다. 이 시스템은 적응적 의사결정, 맥락 이해, 반복적인 문제 해결이 필요한 복잡하고 개방적인 작업을 수행하도록 설계되었습니다. 이 논문은 더 복잡한 다중 에이전트(Multi-agent) 시스템의 기본 단위가 되는 단일 에이전트 시스템에 초점을 맞추고 있습니다.

#### 핵심 구성 요소
에이전트 시스템의 중심에는 **기초 모델(Foundation Model)**인 LLM 또는 멀티모달 모델이 추론 및 제어 센터로서 자리 잡고 있습니다. 이 핵심 모델을 보조하며 에이전트의 자율성을 확장하는 세 가지 주요 모듈은 다음과 같습니다.
1. 계획 모듈 (Planning Module): 복잡한 목표를 실행 가능한 단계로 분해하고 실행 순서를 조직합니다.
  - 정적 계획: Chain-of-Thought(CoT)처럼 고정된 추론 경로를 따릅니다.
  - 동적 계획: ReAct나 Reflexion처럼 환경의 피드백이나 과거 행동을 반영하여 계획을 반복적으로 수정합니다.
2. 도구 사용 (Tool Use): 에이전트가 외부 리소스(웹 검색, API, 코드 실행 환경 등)와 상호작용하게 하여 모델 내부 지식의 한계를 확장합니다. 적절한 도구를 선택하고, 실행 결과를 다시 추론 과정에 통합하는 능력이 핵심입니다.
3. 메모리 모듈 (Memory Module): 과거 정보를 저장하고 인출하여 장기적인 일관성과 맥락 인식 능력을 갖추게 합니다.
    ◦ 단기 기억: 현재 수행 중인 작업의 맥락 정보를 저장합니다.
    ◦ 장기 기억: 여러 세션에 걸쳐 지속되는 지식과 경험을 축적하며, 주로 검색 증강 생성(RAG) 메커니즘을 통해 필요한 정보를 가져옵니다.

## Adaption
인공지능 시스템이 특정 도메인, 작업 또는 운영 환경의 요구 사항에 더 잘 맞도록 자신의 행동, 의사결정 전략 및 내부 표현을 조정할 수 있게 하는 필수적인 과정입니다.
적응 방식은 크게 모델의 파라미터를 건드리지 않는 방식(Prompt Engineering) 과 직접 수정하는 방식(Fine Tuning)의 두 가지 범주로 나뉩니다.

### 프롬프트 엔지니어링 (Prompt Engineering)
프롬프트 엔지니어링은 기초 모델의 파라미터를 수정하지 않고 에이전트의 행동을 유도하는 가벼운(lightweight) 형태의 적응 방식입니다.
- 작동 방식: 모델을 재학습시키는 대신 목표, 제약 조건, 문맥적 지침이 담긴 정교한 입력 프롬프트를 설계하여 에이전트의 행동을 형성합니다.
- 주요 구성: 일반적으로 지침(instructions), 예시(examples), 작업 설명(task descriptions) 등으로 구성되며, 이를 통해 모델이 특정 추론 패턴이나 행동 전략을 취하도록 유도합니다.
- 장점: 추가적인 모델 학습이 필요 없어 매우 효율적이며, 서로 다른 작업 간에 쉽게 전이될 수 있다는 장점이 있습니다. CAMEL, AutoGen, ChatDev와 같은 최근의 많은 에이전트 시스템이 이 방식을 채택하고 있습니다.

### 파인튜닝 (Fine-Tuning)
프롬프트 엔지니어링과 대조적으로, 파인튜닝은 모델의 내부 파라미터를 직접 업데이트하여 적응을 달성합니다.

- 작동 방식: 작업 특화된 데이터(task-specific data)에 모델을 노출시켜 대상 도메인의 목적에 부합하는 새로운 지식, 추론 패턴, 행동 경향을 내재화하도록 만듭니다.
- 적용 수준:
  - 전체 파인튜닝(Full Fine-Tuning): 모델의 모든 파라미터를 업데이트하여 유연성을 극대화하지만, 막대한 계산 자원이 필요합니다.
  - 매개변수 효율적 파인튜닝(PEFT): LoRA(Low-Rank Adaptation)와 같이 전체 파라미터 중 아주 작은 부분만 업데이트하여 효율성과 성능 사이의 균형을 맞춥니다.

## 프레임 워크 제시
에이전트 AI의 적응(adaptation)을 두 가지 주요 기준에 따라 네 가지 핵심 패러다임으로 나눕니다.

### 첫 번째 기준: 최적화 대상 (Optimization Target)
- 에이전트(Agent): 에이전트 모델 자체의 내부 매개변수, 표현 방식, 또는 행동 정책을 수정하여 작업 요구사항에 더 잘 맞도록 합니다.
- 도구(Tool): 에이전트가 사용하는 외부 도구(예: 검색기, 플래너, 메모리 모듈, 특화 모델)를 최적화하여, 에이전트가 고정된 상태에서도 적응형 운영 환경의 이점을 누릴 수 있도록 합니다.


### 두 번째 기준: 적응 신호 유형 (Type of Adaptation Signal)
- 에이전트: 최적화 신호가 도구 실행 피드백에서 오는지, 아니면 에이전트 자체의 최종 출력 평가에서 오는지에 따라 나뉩니다.
- 도구: 도구가 에이전트와 독립적으로 최적화되는지, 아니면 고정된 에이전트의 감독 하에 적응되는지에 따라 나뉩니다.


### 패러다임 1 - A1: Tool Execution Signaled Agent Adaptation (도구 실행 신호 기반 에이전트 적응)
이 방식에서 에이전트가 학습하는 데 사용되는 피드백은 도구의 실행 결과에서 직접 발생합니다. 예를 들어, 에이전트가 생성한 코드가 성공적으로 실행되었는지, 혹은 검색 질의(query)가 얼마나 관련성 높은 문서를 반환했는지 등이 피드백으로 활용됩니다.
이 피드백은 지도 학습(Supervised Learning) 및 강화 학습(Reinforcement Learning) 모두를 구동하는 데 사용될 수 있는 "검증 가능한 신호(verifiable signal)"를 형성합니다.

- 최적화 대상: 에이전트(A)
- 적응 신호: 에이전트가 호출한 외부 도구(T)의 실행 결과로부터 얻은 검증 가능한 피드백(예: 코드 샌드박스 결과, 검색 관련성 점수, API 호출 결과)

#### SFT & Off-Policy Methods
미리 수집된 데이터(궤적 또는 응답)를 바탕으로 SFT이나 DPO와 같은 오프-폴리시(Off-Policy) 방식을 사용해서 에이전트에 Adaption 을 적용합니다.

Toolformer (2023): 이 분야의 선구적인 모델로, 도구 실행 결과를 자기 지도 학습(Self-supervised) 신호로 사용하는 개념을 도입했습니다. 모델이 텍스트에 API 호출을 삽입하고 실행한 뒤, 그 결과가 다음 토큰 예측의 혼란도(Perplexity)를 얼마나 낮추는지를 측정하여 유용한 호출만을 학습 데이터로 남깁니다.

##### 학습 신호의 3단계 진화 과정
논문은 A1 유형 방법들이 더 정교하고 신뢰할 수 있는 학습 신호를 얻기 위해 세 가지 단계를 거쳐 진화했다고 설명합니다.

<b> 1단계: 정답(Golden Answers)과의 정렬 </b>  
모델을 올바른 최종 출력, 즉 태스크별 정답이나 검증된 전문가 솔루션에 맞춰 학습하는 방식입니다.

- TRICE: 2단계 프레임워크로, 먼저 SFT로 기초 능력을 쌓은 뒤 **'실행 피드백 기반 강화학습(RLEF)'**을 통해 도구 실행 결과가 실제 정답과 일치하는지 점수를 매겨 모델을 강화합니다.
- ToolAlpaca: 생성-실행-평가-파인튜닝의 **폐쇄 루프(Closed-loop)**를 구축하여, API 호출 결과(반환 값, 오류 등)를 통해 모델이 도구 사용 능력을 스스로 개선하도록 합니다.
- TP-LLaMA: 성공한 경로뿐 아니라 **실패한 경로(errors)**에서도 학습합니다. 전문가의 올바른 단계와 실패한 단계를 비교하는 선호도 데이터셋을 만들어 DPO로 학습시킴으로써 실패로부터 배우는 능력을 강화합니다.

<b> 2단계: 표준 형식(Golden Formats)과의 정렬 </b>  
출력 결과의 정확성뿐만 아니라 도구 호출의 구문적, 논리적 유효성에 중점을 둔 방식으로, 구조적 정확성을 강조합니다.

- Gorilla: 추상 구문 트리(AST)를 사용하여 모델이 생성한 API 호출이 문법적으로 올바른지 확인합니다. 이는 단순 텍스트 매칭보다 견고한 평가 방식이며, 에이전트가 올바른 도구 사용 형식을 갖추도록 유도합니다.
- ToolFlow: 그래프 기반 샘플링과 계획된 생성을 통해 자연스럽고 일관된 다회차(Multi-turn) 도구 호출 대화 데이터를 합성하여 학습에 활용합니다.

<b> 3단계: 직접 도구 실행(Direct Tool Execution)과의 정렬 </b>  
인간의 주석 없이 검증 가능한 환경 신호 자체를 감독 신호로 삼는 가장 진보된 단계입니다. 

- CodeAct: 텍스트 명령 대신 실행 가능한 코드 액션을 생성하고, 샌드박스 환경에서 돌아온 성공/실패 신호를 직접 감독 신호로 사용하여 에이전트의 행동을 도구의 인과 기제와 일치시킵니다.
- NExT: 프로그램 수정 작업을 위해 실행 결과를 필터링 신호로 사용합니다. 유닛 테스트를 통과한 결과물만을 모아 에이전트를 반복적으로 학습시키는 '샘플-필터-학습' 루프를 제안했습니다.
- AutoTools: 도구 캡슐화와 프로그래밍을 자동화하여, API 문서를 스스로 파싱하고 실행 가능한 프로그램을 생성하며 실행 결과로부터 스스로 개선하는 자율적 도구 학습을 지향합니다.
- LeReT & RetPO: 검색(Retrieval) 영역에서 활용됩니다. LeReT는 검색 쿼리의 품질을 보상으로 삼아 IPO 알고리즘으로 
최적화하며, RetPO는 off-the-shelf 검색기의 성능을 보상으로 사용하여 쿼리 재구성 모델을 학습시킵니다.

##### 결론 및 한계
A1 방식의 초기 연구들은 '암묵적인 자기 지도 피드백'에서 '명시적인 실행 기반 학습 신호'로 점진적으로 이동해 왔습니다. 이러한 과정은 모델의 추론과 환경 상호작용을 더 밀접하게 결합시켜 도구 사용의 신뢰성을 높였으나, 여전히 미리 수집된 데이터(SFT/DPO)에 의존하기 때문에 능동적인 탐험(Exploration) 능력이 제한적이라는 한계가 있습니다

#### RLVR-Based Methods
이전의 SFT(Supervised Fine-Tuning) 또는 DPO(Direct Preference Optimization)와 같은 방법들은 미리 수집된 데이터나 후보 응답에 의존하여 모델을 업데이트 한 반면, RLVR 방법은 LLM을 활용해 환경과 상호작용하며, 반복적으로 행동을 탐색하고, 실행하며, 즉각적인 환경 피드백을 기반으로 자신의 행동을 정교화할 수 있도록 합니다. 이를 통해 특정 문맥(context)을 인지하여 행동을 조정하거나 특정 실행 환경과 긴밀하게 연동되어 최적의 성능을 발휘할 수 있는 여건을 마련할 수 있게 됩니다.

RLVR은 에이전트가 도구 및 환경과의 직접적인 상호작용을 통해 학습하고 행동을 개선하는 방식이기 때문에, 결과를 명확하게 검증할 수 있는(verifiable) 다양한 복잡한 작업에 매우 효과적으로 사용됩니다. RLVR은 결과를 객관적으로 평가할 수 있는 다양한 복잡한 문제 해결 영역에서 에이전트의 성능, 신뢰성 및 일반화 능력을 향상시키는 데 강력한 도구로 활용되고 있습니다.

##### RLVR 적용분야-1 : 웹 검색 및 정보 검색 도구 (Web search and information retrieval tools)

목표: 사용자 질의에 대해 최적의 검색 질의를 생성하고, 관련 문서를 효율적으로 검색하는 능력을 향상시킵니다.
작동 방식: 에이전트가 검색 질의를 재구성하는 것을 MDP(Markov Decision Process)로 모델링합니다. 사용자의 원본 질의가 '상태(state)'가 되고, 재구성된 질의가 '행동(action)'이 됩니다. 검색 결과(예: Recall@K, NDCG, SQL 실행 정확도)가 '보상(reward)'으로 활용됩니다.

- DeepRetrieval [21]: LLM을 검색 에이전트로 훈련하여 검색 결과로부터 직접 학습하도록 합니다. 질의 재구성을 MDP로 형식화하고, 검색 효율성과 구문적 유효성을 동시에 고려하는 보상 함수를 사용하여 KL-정규화된 PPO(Proximal Policy Optimization)로 정책을 최적화합니다. 이 방법을 통해 문헌 검색, QA 스타일 검색, 텍스트-SQL 데이터베이스 질의 등에서 성능이 크게 향상되었습니다.
- ReZero [62]: DeepRetrieval의 후속 연구로, 실패한 검색 후 적응형 재시도를 보상하는 GRPO(Group Relative Policy Optimization) 기반 강화 학습을 통해 에이전트의 지속성과 견고성을 향상시킵니다.
Orion [63]: 단일 단계 재구성을 넘어 다중 턴 적응형 검색으로 확장하여, 정규화된 유사도 및 순위에 기반한 턴별 보상을 통해 모델이 구조화된 사고-검색 주기를 통해 반복적으로 검색을 개선하도록 훈련합니다.

##### RLVR 적용분야-2 :코드 기반 도구 (Code-based tools)

목표: 에이전트가 실행 가능한 코드를 생성하고, 코드 실행 환경으로부터 피드백을 받아 코딩 능력을 개선합니다.
작동 방식: 생성된 코드가 샌드박스 환경에서 실행되고, 테스트 케이스 통과율이나 수치적 정확성 같은 실행 결과가 에이전트 정책 최적화를 위한 보상으로 사용됩니다. 이는 결정론적(deterministic)이거나 샌드박스화된 실행 환경을 제공합니다.

- DeepSeek-R1 (Code) [24]: 코드를 생성하고 샌드박스에서 실행하여 얻은 테스트 케이스 통과율이나 수치적 정확성을 정책 최적화를 위한 보상으로 직접 사용합니다.
- LeDex [64]: PPO 기반 알고리즘을 사용하여 코드의 정확성(단위 테스트 결과, CodeBLEU)과 설명의 품질(의미론적 유사성)을 모두 고려하는 새로운 보상 함수로 강화 학습을 적용합니다.
R1-Code-Interpreter [66]: 다단계 강화 학습을 통해 LLM이 코드 인터프리터를 효과적으로 사용하도록 훈련하는 일반적인 프레임워크를 제시합니다.

##### RLVR 적용분야-3 :형식적 정리 증명 (Formal theorem proving)

목표: 에이전트가 수학적 정리나 논리적 명제를 증명하는 과정을 자동화하고 개선합니다.
작동 방식: 증명 보조 도구(formal proof checker)가 에이전트가 제안한 증명 단계(tactic)의 유효성을 결정론적으로 검증합니다. 이 검증 결과(예: tactic이 수락되었는지, 증명 상태가 진전되었는지, 완전한 증명이 달성되었는지)가 검증 가능한 보상 신호로 직접 사용됩니다.
특징: 단위 테스트가 희박하거나 불완전할 수 있는 코드 실행 RLVR과 달리, 정리 증명은 단계별 의미론적 검증을 통해 모호성이 적고 더 밀집된 보상(denser rewards)을 제공하여 장기적인 귀인(long-horizon credit assignment)을 상당히 용이하게 합니다.

- AlphaProof [77], DeepSeek-Prover-V2 [78], Kimina-Prover [72] 등: 검증자(verifier) 피드백을 활용하여 다단계 증명 검색 정책을 강화 학습을 통해 훈련합니다.

##### RLVR 적용분야-4 : 멀티-도구 추론 시스템 (Multi-tool reasoning systems)

목표: 에이전트가 여러 도구를 순차적으로 또는 복합적으로 사용하여 복잡한 작업을 해결하고, 환경 피드백을 통해 도구 선택 및 활용 전략을 개선합니다.
작동 방식: 에이전트가 다양한 LLM(라우팅 풀), API, 또는 기타 외부 도구를 동적으로 호출하고, 각 호출의 성공 여부나 전체 작업의 최종 결과에 따라 보상을 받습니다.

- Router-R1 [87]: 여러 LLM을 조정하는 정책 LLM을 강화 학습 프레임워크를 통해 훈련합니다. 정책 LLM은 내부 추론과 외부 모델 선택 사이를 전환하며 복잡한 작업을 해결합니다.
- FTRL [88]: 검증 가능한 보상 함수를 사용하여 모델의 도구 사용 능력을 향상시키는 피드백 기반 훈련 프레임워크를 소개합니다.
- Tool-N1 [25]: 멀티-도구 추론 시나리오에서 도구 호출 능력을 향상시키기 위해 R1 스타일 강화 학습으로 훈련된 LLM 시리즈입니다.

##### 기타 적용 분야

- 추천 시스템 (Recommendation Optimization):
Rec-R1 [91]: LLM을 추천 시스템에 직접 최적화하여 하류 추천 시스템의 피드백을 사용하여 LLM을 훈련합니다. LLM 생성을 정책으로, 추천 지표(예: NDCG, Recall)를 보상 신호로 사용하여 생성물을 실제 추천 성능에 맞춥니다.
- 텍스트-SQL (Text-to-SQL):
SQL-R1 [92]: NL2SQL(Natural Language to SQL) 작업을 다루며, 형식, 실행, 결과, 길이 보상으로 구성된 맞춤형 RL 보상 함수를 통해 모델이 사용자 의도를 정확하게 반영하는 SQL 질의를 생성하도록 안내합니다.
- 문서 OCR (Document OCR): olmOCR 2 [93]: 디지털화된 인쇄 문서를 자연스럽게 정렬된 일반 텍스트로 변환하는 OCR 시스템입니다. 기존 편집 거리 측정 방식 대신 텍스트 존재/부재, 읽기 순서, 표 정확도, 수학 공식 렌더링 등을 포함하는 다양한 이진 단위 테스트를 보상 신호로 사용하여 모델을 훈련합니다.

##### 주요 특징 및 이점
• 시행착오를 통한 학습: 모델은 실시간 환경 피드백을 통해 **"무엇이 실제로 작동하는지"**를 직접 경험하며 추론 및 도구 사용 전략을 정교화합니다.
• 검증 가능성: 단순한 인간의 선호도가 아닌, 코드 실행 성공이나 수학적 정답 여부와 같이 객관적이고 검증 가능한 신호를 학습에 활용합니다.
• 한계점: 보상 설계(Reward Design)가 매우 까다롭고, 상호작용을 위한 막대한 계산 자원이 필요하며 학습 안정성을 높이는 메커니즘이 필수적입니다

### 패러다임 2 - A2: Agent Output Signaled Agent Adaptation (에이전트 출력 신호 기반 에이전트 적응)
A2 (에이전트 출력 신호 기반 에이전트 적응) 패러다임은 에이전트가 외부 도구를 실행한 결과 자체가 아닌, 에이전트가 내놓은 최종 출력물(답변, 계획, 추론 경로 등)에 대한 평가를 최적화 신호로 사용하여 에이전트 모델을 개선하는 방식입니다.

- 최적화 대상: 에이전트(A)
- 적응 신호: 에이전트 자체의 최종 출력(예: 최종 답변, 계획, 추론 과정)에 대한 평가로부터 얻은 신호. 도구 사용 여부와 상관없이 최종 출력의 품질이 중요합니다.

#### Agent Adaptation w/o Tools (도구 없는 에이전트 적응
외부 도구의 도움 없이 에이전트 내부의 내재적인 추론 능력(수학, 코딩, 논리적 추론 등)을 강화하는 데 집중하는 연구 분야

##### R1 패러다임의 등장:
DeepSeek-R1 프레임워크와 Kimi-1.5를 통해 **검증 가능한 보상 기반 강화 학습(Reinforcement Learning with Verifiable Reward, RLVR)**이 대형 에이전트의 추론 능력을 효과적으로 향상시킬 수 있음이 입증되었습니다.
특히 수학 및 코드 생성과 같이 결과물의 품질을 결정론적 정확도 신호(deterministic correctness signals)로 자동 평가할 수 있는 추론 집약적 영역에 초점을 맞춥니다.
이는 기존의 지도 미세 조정(Supervised Fine-Tuning, SFT)을 넘어 에이전트 지능을 향상시키는 확장 가능한 경로를 제시하며 'R1 패러다임'이라는 새로운 연구 흐름을 촉발했습니다.


##### R1 패러다임 확장 연구:
Empower는 명시적 정확성 대신 '인간 역량 강화(human empowerment)'를 목표로 하는 자체 지도 미세 조정(self-supervised fine-tuning) 프레임워크를 제안했습니다.
KnowRL은 에이전트가 자신의 지식과 답변의 신뢰도를 스스로 평가하도록 강화 학습을 사용합니다.
GRACE는 대조 학습(contrastive learning)을 보상 기반 최적화 형태로 재해석하여 명시적 추론을 유도합니다.
EHRMind는 RLVR을 임상 추론 시나리오에 적용하며, 도메인 특화 추론에는 SFT를 통한 사전 지식 정렬이 중요함을 강조합니다.

#### Agent Adaptation w/ Tools (도구 활용 에이전트 적응)
에이전트의 최종 출력물(답변, 계획 등)을 최적화 신호로 삼아, 에이전트가 도구를 언제, 어떻게 사용할지 결정하는 '전략적 정책'을 강화하는 연구 분야입니다.

##### 검색 기반 도구 학습 (Retrieval-based Tool Learning)
최근 연구들은 강화학습(RL)을 통해 에이전트가 다단계 추론 과정에서 자율적으로 검색 쿼리를 생성하고 정제하는 능력을 배양하는 데 집중하고 있습니다.
• R1-Searcher & Search-R1: 검색된 증거와 최종 답변의 정답 여부를 결합하여 보상을 제공함으로써, 에이전트가 추론과 검색 사이의 균형을 맞추도록 학습시킵니다. 이를 통해 사실 관계의 정확성을 높이고 환각(hallucination) 현상을 최대 24%까지 줄였습니다.
• ReSearch: 지도 학습 데이터 없이 오직 강화학습(GRPO)만으로 학습하며, <think>, <search>, <result> 태그를 사용하여 추론 체인 안에 검색 과정을 통합합니다. 이 과정에서 에이전트는 검색이 필요한 시점을 스스로 판단하는 자기 성찰 및 자기 교정 행동을 자연스럽게 습득하게 됩니다.
##### 코드 및 실행 기반 도구 학습 (Code- and Execution-based Tool Learning)
코드를 도구로 사용하는 환경에서 에이전트의 최종 출력을 개선하기 위한 시도들입니다.
• CodePRM: 코드 실행 결과를 바탕으로 추론의 각 단계에 점수를 매기는 **과정 보상 모델(Process Reward Model)**을 도입하여, 추론 오류를 동적으로 수정하는 '생성-검증-정제' 파이프라인을 구축합니다.
• ReTool: 실시간 코드 실행을 강화학습 과정에 통합하여, 모델이 수학적 또는 상징적 추론을 위해 언제 계산기나 인터프리터를 호출해야 하는지를 학습하게 합니다.
##### 범용 다중 도구 및 에이전트 학습 (General Multi-tool and Agentic Learning)
다양한 API와 상호작용하는 복잡한 워크플로우를 최적화하는 포괄적인 프레임워크들입니다.
• Agent-R: 몬테카를로 트리 탐색(MCTS)과 모델 기반 비판(Critique) 구성을 통해 실패한 경로를 지속적으로 수정하며 성능을 높입니다.
• A2FM: 추론과 행동을 비용 조절형 강화학습(Cost-regularized RL) 프레임워크 내에서 통합하여, 내부 추론을 할지 외부 도구를 쓸지 동적으로 선택함으로써 효율성과 정확성을 동시에 개선합니다.
• VerlTool: 서로 다른 작업을 수행하는 다중 에이전트와 도구 서버를 분리하여 대규모 강화학습 학습을 가능하게 하는 효율적인 인프라를 제공합니다.

##### 요약 및 의의
이 패러다임은 단순히 도구 사용법을 익히는 수준(A1)을 넘어, 에이전트가 도구를 지능적으로 조율(Orchestration)하는 상위 인지 능력을 내재화하게 합니다. 비록 학습 비용이 높고 보상이 희소(sparse)하다는 단점이 있지만, 복잡한 워크플로우에서 에이전트의 전반적인 전략을 최적화하는 데 매우 효과적입니다

### T1: Agent-Agnostic Tool Adaptation (에이전트 독립적인 도구 적응)
기존에는 에이전트 자체(예: 에이전트의 내부 파라미터)를 최적화하는 데 집중했지만, 도구 적응은  에이전트의 내부 인지 능력을 수정하는 대신, 에이전트가 사용하는 외부 생태계(도구)를 최적화하여 성능을 높이는 전략을 다룹니다. 이는 에이전트의 파라미터를 고정(Frozen)한 채로 주변 환경을 개선하여 공생적인 에이전트-도구 생태계를 구축하는 데 중점을 둡니다. 이는 미리 학습된(pre-trained) 모델, 검색기(retrievers), 플래너(planners), 실행기(executors) 등을 아우르며, 에이전트는 언어나 코드를 통해 이들을 호출하여 사용합니다.

- 최적화 대상: 도구(T)
- 적응 신호: 에이전트와는 독립적으로 도구 자체의 성능(예: 검색 정확도, 순위 품질)을 기반으로 도구를 훈련합니다.

#### Foundational Systems and Architectures
에이전트가 외부 도구를 효과적으로 활용할 수 있도록 하는 '기반 시스템 및 아키텍처'에 대해 설명합니다. 즉, 고정된 에이전트가 외부 도구를 어떻게 조정(Orchestration)하고 호출(Invocation) 하는지에 대한 메커니즘이나 프레임워크를 소개 합니다.

##### 1. 연산자 학습 도구 (Operator-Learning Tools)
대규모 LLM 기반 오케스트레이션이 등장하기 전, 복잡한 시뮬레이터를 위한 미분 가능한 대리 모델(differentiable surrogates) 역할을 하는, 무한 차원 함수 공간 간의 매핑을 근사하도록 훈련된 모델입니다.  
이는 에이전트가 재훈련 없이 반복적으로 쿼리할 수 있는 "고정된 도구"의 초기 예시를 제공하여, 에이전트의 추론, 계획, 제어 루프 내에서 빠르고 미분 가능한 블랙박스 함수로 사용될 수 있는 기반을 마련했습니다.

##### 2. HuggingGPT: 프롬프트 기반 조정
ChatGPT가 HuggingFace Hub의 1000개 이상의 머신러닝 모델을 파인튜닝 없이 제어할 수 있도록 한 시스템입니다. 작업 계획, 모델 선택, 작업 실행, 응답 생성의 4단계 워크플로우를 통해 언어가 범용적인 인터페이스 역할을 할 수 있음을 보여주었습니다. 복합적인 교차-모달(cross-modal) 작업에서 GPT-3.5가 GPT-4V에 필적하는 성능을 보이도록 했습니다.

##### 3. ViperGPT: 코드 생성 기반 조정
ViperGPT는 고정된 LLM이 텍스트 명령 대신 실행 가능한 파이썬 코드를 생성하여 도구들을 조합하는 방식을 도입했습니다. 생성된 파이썬 코드가 GLIP(탐지), SAM(세분화) 등 다양한 시각 모델을 호출하여 복잡한 시각적 추론 작업을 수행합니다.    
단순 API 호출보다 파이썬 함수를 통한 구성이 더 유연하다는 통찰을 주었으며, 구성적(compositional) 작업에서 뛰어난 성능을 보였습니다.

##### 4. SciToolAgent: 그래프 기반 조직
과학 분야로 도구 오케스트레이션을 확장한 시스템입니다. 고정된 GPT-4o가 SciToolKG라는 지식 그래프를 통해 500개 이상의 생물학, 화학, 재료 과학 도구에 접근합니다. 지식 그래프를 활용한 도구 선택으로 과학 쿼리 벤치마크에서 94%의 정확도를 달성하여, 구조화된 지식 그래프가 프롬프트 기반 설명의 확장성 문제를 해결할 수 있음을 입증했습니다.

##### 5. 모델 맥락 프로토콜 (Model Context Protocol, MCP)
에이전트가 이기종 도구들과 통신하는 방식을 표준화하는 개방형 규약입니다. 도구 정의를 모델의 컨텍스트(입력창)에 일일이 집어넣는 대신, 표준화된 API 레이어를 통해 도구를 발견하고 호출합니다. 이를 통해 Anthropic의 "MCP 기반 코드 실행" 방식은 에이전트가 외부 시스템과 일관된 스키마를 통해 인터페이스할 수 있도록 하여, 긴 도구 정의나 중간 결과를 직접 삽입하는 대신 필요한 도구 정의만 로드하고 샌드박스 환경 내에서 데이터를 필터링하거나 집계할 수 있게 함으로써 컨텍스트 사용량을 98% 이상 줄였습니다. 

#### Categories and Training Methodologies
고정된 에이전트가 즉시 연결하여 사용할 수 있는 '플러그 앤 플레이(Plug-and-Play)' 도구의 범주와 이를 구축하기 위한 학습 방법론을 다룹니다. 즉, 이 도구들은 특정 에이전트에 종속되지 않고 각각 고유한 기능적 능력을 가지고 있으며, 다양한 데이터 소스에서 에이전트와 독립적으로 미리 훈련(pre-trained)되어 있습니다. 즉, 에이전트가 어떤 프레임워크 안에서 가져다 쓰는 '도구 자체'의 종류와 그 도구들이 어떻게 학습되었는지에 집중합니다.

##### Vision models (시각 모델)
예시: CLIP, SAM, SAM-CLIP
특징: 이 모델들은 방대한 이미지-텍스트 쌍(CLIP)이나 대규모 이미지 데이터(SAM)로 훈련되어 제로-샷(zero-shot) 분류, 의미론적 이해, 객체 분할과 같은 다양한 시각 작업을 수행합니다.
활용: 에이전트는 이러한 툴들을 API 호출을 통해 직접 사용하며, 툴 자체의 추가적인 미세 조정(fine-tuning) 없이 이미지 이해 및 분석 작업을 처리할 수 있습니다.

##### Speech and audio tools (음성 및 오디오 툴)
예시: Whisper
특징: 수십만 시간의 다국어 오디오 데이터로 훈련되어 음성 인식, 번역, 언어 식별 등 강력한 성능을 보여줍니다.
활용: 에이전트는 음성 입력값을 동결된(frozen) Whisper 모델에 전달하고 텍스트 출력을 받아 처리하며, 에이전트가 툴 모델을 추가로 적응시킬 필요가 없습니다.

##### Code execution tools (코드 실행 툴)
예시: CodeAct
특징: 툴 사용을 텍스트나 JSON 기반 명령 대신 실행 가능한 Python 코드로 표현함으로써, 에이전트가 유연하게 툴을 구성하고 매개변수를 지정하며 여러 툴을 결합할 수 있게 합니다.
활용: 이러한 툴들은 코드 실행 환경에서 작동하여 에이전트의 복합적인 추론 능력을 향상시킵니다.

##### Search and retrieval tools (검색 및 검색 툴)
예시: DPR, ColBERT, Contriever, e5
특징: 주로 패시지 랭킹(passage ranking) 작업에 대해 미리 훈련된 바이-인코더(bi-encoder) 모델입니다.
활용: 대규모 코퍼스에 대한 의미론적 검색을 가능하게 하여, 에이전트가 필요한 정보를 효율적으로 검색하고 활용할 수 있도록 돕습니다.

##### Scientific tools (과학 툴)
예시: AlphaFold2, ESMFold, CGCNN, 분자 표현 학습(Molecule representation learning) 방법
특징: 단백질 구조 예측, 결정 특성 예측, 분자 특성 예측 등 특정 과학 분야에서 오랜 기간 개발된 전문 모델들입니다.
활용: 동결된 에이전트가 과학적 질의를 처리할 때 이 툴들을 그대로 활용하여 도메인 특화된 계산 및 분석을 수행합니다.

##### Adaptive agents as T1 tools (A1 방식의 적응형 에이전트가 T1 툴이 되는 경우)
예시: DeepRetrieval, Code-R1
특징: 이전에 A1 패러다임(툴 실행 결과로 신호를 받는 에이전트 적응)으로 훈련되어 특정 작업을 수행하도록 학습된 에이전트 모델들입니다.
활용: 이들 모델이 일단 훈련되고 동결되면, 다른 에이전트 시스템에서 "툴"로서 활용될 수 있습니다. 예를 들어, DeepRetrieval은 검색 질의를 다시 작성하는 툴로, Code-R1은 실행 가능한 코드를 생성하는 툴로 사용될 수 있어, 미리 훈련된 모델과 동적 툴 간의 간극을 메웁니다.



### T2: Agent-Supervised Tool Adaptation (에이전트 감독형 도구 적응)

최적화 대상: 도구(TTTT)
적응 신호: 고정된 에이전트(AAAA)의 출력에서 파생된 신호를 사용하여 도구를 적응시킵니다.
설명: 에이전트 자체는 고정되어 있지만, 에이전트의 최종 출력을 통해 도구가 최적화되어 에이전트의 전반적인 능력 향상을 목표로 합니다.

이 네 가지 패러다임은 상호 배타적이지 않으며, 최첨단 시스템에서는 여러 적응 패러다임을 조합하여 최적의 성능을 달성하는 경우가 많습니다.


## 기여
에이전트 인공지능 적응 전략에 대한 최초의 포괄적인 분류 체계를 제공하며, 각 범주의 기술적 조사, 전략별 비교 분석, 실제 응용 사례(딥 리서치, 소프트웨어 개발 등), 그리고 향후 연구 기회를 제시하는 것을 목표로 합니다.
