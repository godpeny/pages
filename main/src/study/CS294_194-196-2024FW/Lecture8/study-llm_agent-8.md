# CS294/194-196 LLM Agent: 8. Neural & Symbolic Decision Making Framework

## Problems of LLMs
LLM은 대화형 AI나 콘텐츠 생성 등에서 뛰어난 능력을 보이지만, 복잡한 추론 및 계획(Reasoning and Planning) 문제에서는 여전히 한계를 보입니다. OpenAI의 o1 모델과 같은 최신 추론 모델들도 문제의 복잡도가 커지면 성능이 급격히 떨어지는 경향을 보입니다. 가령, 여행 계획(Travel planning)과 같이 수많은 제약 조건을 만족해야 하는 문제에서 최신 모델인 GPT-4 Turbo조차 최종 성공률(Final Pass Rate)이 0.6%에 불과하며, 도구 사용 정보가 주어져도 4.4% 수준에 머뭅니다.

### Retrieval vs Reasoning?
LLM이 진정한 추론을 하는 것이 아니라 학습 데이터에서의 "근사적 검색(Approximate retrieval)"에 의존하고 있다는 비판이 존재하며, 무관한 문장이 삽입될 경우 성능이 크게 저하되는 특성을 보입니다.

## Solutions Option 1: Scaling Law
더 많은 데이터, 더 많은 계산 자원, 더 큰 모델을 사용하는 전통적인 방식입니다. 하지만 매우 비용이 많이 들며, 엔지니어링 측면의 도전 과제는 해결할 수 있으나 추론과 계획 문제에 대한 근본적인 사고의 전환을 제공하지는 못할 수 있습니다.

## Solutions Option 2: Hybrid System
언어 이해에 능숙한 딥러닝 모델과 최적의 해를 보장하는 심볼릭 솔버(Symbolic Solver)를 결합하는 방식입니다. 이 심볼릭 솔버는 도구로 활용(Tool Use) 또는 데이터 제공자로 활용(Provide Data) 로 크게 두 가지 방식으로 LLM과 연결됩니다.

### 1. Tool Use
LLM이 직접 계획을 세우는 대신, 사용자의 자연어 요청을 JSON 형태의 '심볼릭 언어'로 번역하면, 심볼릭 솔버가 이 데이터를 도구로서 전달받아 최적의 계획(예: 여행 일정)을 계산해 줍니다. 이 경우 솔버는 외부 도구와 같은 개념입니다.

### APEC-Travel
자연어 입력을 JSON 형태의 심볼릭 묘사로 변환한 뒤, 이를 혼합 정수 선형 계획법(MILP) 솔버에 입력하여 최적의 경로를 실시간으로 찾아냅니다.
1. 사용자가 "예산 1000달러로 3일간 샌프란시스코 여행을 가고 싶어"라고 요청하면, 파인튜닝된 LLM(Instruction Translator)이 이 요청을 JSON 형태의 심볼릭 묘사로 변환합니다.
2. 변환된 JSON 데이터와 데이터베이스의 항공/호텔 정보를 혼합 정수 선형 계획법(MILP) 솔버에 입력합니다.
3. 심볼릭 솔버는 주어진 예산, 숙소 규칙 등 모든 제약 조건을 만족하는 수학적으로 최적화된 실행 가능한 계획을 실시간으로 도출합니다.


### 2. Provide Data
Symbolic Solver가 생성한 탐색 과정(Trace) 데이터를 모델 학습에 활용하는 방식입니다.

#### Search Augmented vs Solution Only
- Solution Only: 문제(쿼리)에서 바로 최종 정답을 예측하도록 학습하는 일반적인 방식입니다.
- Search Augmented: Solver가 생성한 탐색 과정(Trace)과 최종계획(Plan) 데이터를 Model에 전달합니다. Model은 이 문제를 설명하는 프롬프트를 입력받아 Solver의 탐색 과정을 한 토큰씩 예측하도록 교사 학습(Teacher Forcing) 방식으로 학습됩니다. 학습이 완료된 모델은 새로운 문제를 접했을 때, 단순히 정답만 내놓는 것이 아니라 솔버가 했던 것처럼 논리적인 탐색 과정을 먼저 생성한 뒤 최종적인 최적의 계획을 도출하게 됩니다.

Search Augmented 모델은 Solution Only 모델보다 파라미터 수와 데이터 효율성 면에서 훨씬 뛰어납니다. 예를 들어, 15M 파라미터의 탐색 증강 모델이 175M의 솔루션 전용 모델보다 더 적은 데이터로 더 높은 성능을 낼 수 있습니다.

#### SearchFormer
A* 탐색과 같은 알고리즘의 실행 과정을 토큰 예측 작업으로 모델링한 것입니다. Search Augmented 방식으로 학습 한뒤 부트스트래핑으로 파인튜닝 합니다.

#### 부트스트래핑(Bootstrapping)
A* 알고리즘을 모방하는 데 그치지 않고, 모델이 생성한 여러 경로 중 최적해를 찾으면서도 길이가 더 짧은(효율적인) 탐색 과정을 골라 다시 학습시킴으로써 원래 알고리즘보다 더 뛰어난 성능을 갖게 됩니다.

#### DualFormer(SearchFormer v2)
탐색 트레이스의 일부를 무작위로 누락(Dropping)시키는 전략을 사용하여 학습시킨 모델입니다. 누락 전략은 아래의 효과를 가져왔다고 합니다.

1. 누락 전략을 통해 모델은 다양한 수준의 논리 전개 과정을 학습합니다. 그 결과, 모델은 문제의 난이도에 따라 스스로 모드를 전환하는 능력을 갖추게 됩니다.
- 쉬운 문제: 트레이스가 많이 누락된 데이터를 통해 학습한 덕분에, 긴 추론 과정 없이 즉시 정답을 내놓는 시스템 1(Fast Mode)로 작동합니다.
- 어려운 문제: 전체 트레이스 데이터를 통해 학습한 논리를 활용하여, 차근차근 탐색 과정을 거쳐 정답을 도출하는 시스템 2(Slow Mode)로 작동합니다.  

2. SearchFormer에서 트레이스의 불필요한 부분(비용 계산 토큰, 단순 노드 확장 기록 등)을 누락시켜 학습함으로써, 훨씬 짧고 간결한 추론 과정만으로도 최적의 해에 도달하게 됩니다.

3. 트레이스의 다양한 지점을 무작위로 누락시키면 모델이 정답에 이르는 경로를 여러 방식으로 학습하게 됩니다. 이로 인해 모델이 생성하는 해의 다양성(Diversity)이 크게 높아집니다.

4. 트레이스를 완전히 다 보여주거나 아예 안 보여주는 극단적인 방식보다, 다양한 수준으로 누락된 트레이스들을 섞어서 학습시키는 것이 모델의 파라미터 및 데이터 효율성을 가장 높이는 방법임이 증명

### 3. E2E
딥러닝 모델과 심볼릭 솔버를 하나의 통합된 신경망 내에서 동시에 학습시키는 방식입니다. 이 방식은 단순히 솔버를 도구로 쓰거나 데이터를 받는 수준을 넘어, 솔버 자체가 신경망의 한 층(Layer)처럼 작동하여 전체 시스템이 목적 함수를 최적화하도록 설계됩니다.

#### Nonlinear objective with combinatorial Constraints
실세계의 많은 문제는 목적 함수는 비선형적이지만, 동시에 조합론적 제약 조건(Combinatorial Constraints)을 만족해야 하는 특성을 가집니다. 이런 문제는 기존의 솔버를 이용한 방식으로는 풀기가 힘듭니다.  
예를 들어 "GPU에 테이블을 배치할 때 지연 시간을 최소화하라"는 문제는 지연 시간 계산 공식이 매우 복잡(비선형)하고, 테이블은 특정 GPU에 있거나 없거나(이산적 0/1) 해야합니다.

#### Solve the Combinatorial Problem in the Latent Space
비선형 공간에서 문제를 직접 푸는 대신, 잠재 공간(Latent Space)에서 선형 대리(Surrogate) 문제를 구성하여 해결하는 것.

##### Embedding Table Placement
여러 개의 임베딩 테이블을 GPU(장치)들에 할당하되, 각 GPU의 메모리 용량 제한을 지키면서 시스템의 전체 처리 지연 시간(Latency)을 최소화해야 합니다.(비선형적 + 조합론적 문제)

SurCo는 이 문제를 직접 푸는 대신, 딥러닝 모델을 사용합니다.

$\min_x f(x; y) \text{ s.t. } x \in \Omega(y)$ -> $\hat{x}^*(y) = \text{arg}\min_x c(y)^T x \text{ s.t. } x \in \Omega$

먼저 딥러닝 모델(신경망)이 문제의 묘사(테이블 크기, 메모리 제한 등)를 입력받아, 각 테이블을 특정 장치에 할당할 때 발생하는 가상의 비용인 선형 대리 비용 계수(c)를 예측
모델이 예측한 계수 c를 사용하여 단순한 선형 최적화 문제(min c^Tx) 를 구성
MILP 솔버는 메모리 제한 등 모든 조합론적 제약 조건을 엄격히 준수하면서, 예측된 비용 c를 최소화하는 최적의 배치 해(x*)를 도출
솔버가 내놓은 배치 결과(x*)를 원래의 실제 비선형 지연 시간 함수(f(x;y)) 에 넣어 성능을 평가.
여기서 발생한 손실(Loss)은 솔버를 통과하여 역전파(Backpropagation), 모델을 학습 더 최적화된 c를 예측


- 입력(y): 테이블의 메모리 요구량, 각 장치의 메모리 용량 제한 등 문제의 명세가 입력됩니다.
- 출력(c): 모델은 각 테이블을 특정 장치에 할당할 때 발생하는 가상의 비용인 '선형 대리 비용 계수(c)'를 예측합니다.

이 모델이 예측한 c를 사용하여 원래의 비선형 문제를 단순한 선형 최적화 문제($\min \ c^T x$)로 바꿉니다.

즉, GPU(장치)에 임베딩 테이블을 할당하되, 전체 시스템의 처리 지연 시간(Latency)을 최소화하는 문제를 해결 할 때, 딥러닝 모델을 이용해서 이 모델에 인풋으로 제한 사항을 주고, 테이블이 특정 GPU에 할당할때 발생하는 비용 c를 예측하게 하고 이 c를 최소화 하는 문제로 변경한 것입니다.

#### Surco
SurCo의 가장 핵심적인 E2E 특징은 Solver 를 통과하여 역전파할 수 있다는 점입니다.일반적으로 Sover는 미분이 불가능하기 때문에 기존 방식에서는 그래디언트가 모델 내부에서만 흐릅니다. 하지만 Surco는 SurCo는 CVXPYLayers나 Black-box differentiation 기법을 사용하여 솔버의 출력을 미분 가능한 형태로 취급합니다. 이를 통해 최종적인 비선형 손실(Nonlinear Loss)에서 발생한 오차 신호가 솔버를 거쳐, 선형 대리 비용 계수(c)를 생성하는 신경망까지 전달되어 모델을 업데이트합니다.

또 하나 Surco의 특징은 SurCo는 원래의 복잡한 비선형 공간에서의 문제를 직접 푸는 대신, 잠재 공간(Latent Space)에서 솔버가 다룰 수 있는 선형 문제로 변환하여 해결하는 점입니다. 
- 신경망(Neural): 문제의 묘사(y)를 입력받아 솔버가 이해할 수 있는 선형 대리 비용 계수(c)를 예측합니다,.
- 심볼릭 솔버(Symbolic): 신경망이 제공한 계수(c)를 바탕으로 기존의 효율적인 솔버(예: MILP 솔버)를 호출하여 조합론적 제약 조건을 만족하는 최적해 x를 찾아냅니다.

##### Inverse Photonic Design
나노미터 규모의 광학 소자를 설계하는 과정으로, 빛의 파장(Wavelength)에 따라 서로 다른 경로로 빛을 유도하여 정확한 위치로 전달하는 물리적 장치를 만드는 것을 목표로 합니다.

<b> 문제 </b>  
이 문제는 다음과 같은 두 가지 기술적 어려움이 결합된 형태입니다.
- 고도의 비선형 목적 함수: 설계된 소자가 명세를 얼마나 잘 충족하는지 평가하려면 미분 가능한 전자기 시뮬레이터(Electromagnetic Simulator)를 통한 복잡한 시뮬레이션을 거쳐야 합니다. 빛의 간섭과 회절 현상 때문에 이 계산 과정은 매우 비선형적입니다.
- 조합론적 제약 조건 (제조 가능성): 설계는 2D 그리드 상에서 각 셀이 채워져 있거나(1) 비어 있는(0) 이진 형태여야 합니다. 특히, 실제 제조 공정에서 사용하는 '브러시 패턴'(예: 3x3 십자가 모양)으로 그려낼 수 있는 형태여야만 물리적으로 제작이 가능하다는 엄격한 제약 조건이 존재합니다.

<b> 해결 </b>  
3층 합성곱 신경망(CNN)을 사용하여 2D 이미지 형태의 설계 명세를 입력받고, 각 이진 변수에 대한 선형 대리 비용 계수(c)를 예측합니다. 물리적 제약 조건을 준수하는 '브러시 솔버(Brush solver)'를 미분 가능한 레이어처럼 취급하기 위해 블랙박스 미분(Black-box differentiation) 기법을 적용합니다. 이를 통해 시뮬레이션 결과에서 발생한 오차 신호가 솔버를 통과하여 신경망까지 역전파될 수 있습니다.

<b> 성능 </b>  
- 수렴 안정성: 기존의 'Pass-Through' 방식은 수렴 과정이 매우 불안정하고 노이즈가 심한 반면, SurCo-zero는 훨씬 부드럽고 빠르게 최적해에 도달합니다.
- 비약적인 속도 향상: SurCo-prior 방식은 사전 학습된 모델을 활용하여 기존 방식보다 약 100배(2 orders of magnitude) 빠른 속도로 설계 결과를 도출할 수 있습니다.
- 최적의 성능: SurCo-hybrid는 사전 학습된 정보를 바탕으로 실시간 미세 조정을 수행하여, 물리적으로 완벽하면서도 손실이 거의 없는(Loss-0) 설계를 가장 높은 확률로 찾아냅니다.

##### Limitation of SurCo
<b> Vertex Solution </b>  
SurCo는 원래의 비선형 목적 함수 $f(x)$를 직접 푸는 대신, 신경망이 예측한 계수 c를 이용해 선형 함수 $c^Tx$를 최소화하는 문제를 풉니다. 즉, Surco의 Linear Combinatorial Solver가 항상 결정 변수(Decision Variables)와 동일한 차원을 가진 선형 함수 형태로 만들기 때문에 이 선형 함수를 그래프로 그렸을 때 곡선이나 골짜기가 없는 완벽하게 평평한 평면(또는 초평면)이 나타나고, 이때 최소점은 반드시 꼭짓점이게 됩니다. 

$$
-1 \le x_1 \le 1, -1 \le x_2 \le 1, \\[5pt]
f(x_1, x_2) = x_1^2 + x_2^2, \\[5pt]
\min c^T x = \min (c_1x_1 + c_2x_2)
$$

가령 위 문제를 풀때 최적 해는 $(0, 0)$ 이지만, 모델이 주는 $c$가 무엇이든 $(0,0)$은 선택할 수 없게 됩니다. 

<b> SurCo의 시도 </b>  
$\min (c_1x_1 + c_2x_2)$ 를 풀어야 합니다.
- 만약 모델이 $c_1=1, c_2=1$을 주면, 솔버는 가장 구석인 **$(-1, -1)$**을 선택합니다.
- 만약 모델이 $c_1=-1, c_2=1$을 주면, 솔버는 다른 구석인 **$(1, -1)$**을 선택합니다.
- 계수 $c$를 아무리 미세하게 조정하더라도, 솔버의 결과는 항상 **$(-1,-1), (1,-1), (-1,1), (1,1)$** 중 하나의 꼭짓점으로 튕겨 나가게 됩니다.


다만,  많은 실세계 문제(특히 이진 변수 문제)의 최적해가 꼭짓점에 위치하기 때문에 실무적으로는 큰 문제가 되지 않는 경우가 많다고 합니다.

<b> 목적 함수의 미분 가능 </b>  
원래의 비선형 목적 함수 $f$가 해 $x$에 대해 미분 가능해야 Gradient Optimization을 사용할 수 있습니다.

<b> 학습 시 Solver 호출에 따른 계산 비용 </b> 
학습 과정에서 신경망을 업데이트할 때마다 매번 조합 최적화 솔버를 호출해야 합니다. 이는 학습 속도를 늦추는 요인입니다.


## Solutions Option 3: CoGS(Composing Global Solutions)

### Is LLM doing retrieval or true reasoning?
현재 학계에서는 LLM이 진정한 추론(True reasoning)을 하는 것인지, 아니면 단순히 학습 데이터에서 본 내용을 근사적으로 검색(Approximate retrieval)하여 출력하는 것인지에 대한 논쟁이 있습니다. 얀 르쿤(Yann LeCun) 등은 자기회귀형 LLM이 주로 검색 측면에 치우쳐 있다고 주장하며, Apple의 연구(GSM-NoOp)에 따르면 수학 문제에 관련 없는 문장을 섞는 것만으로도 정확도가 급락하는 현상이 발생해 LLM이 논리적 추론보다는 데이터 검색에 의존함을 시사합니다.

### Preliminary
#### Fourie Transform
$z_{pkj} = \sum_{m=0}^{d-1} w_{pmj} e^{imk/d}$

모듈러 연산 문제를 해결하는 신경망의 블랙박스 내부를 들여다보기 위해, 실수로 이루어진 원래의 가중치(w)를 위의 푸리에 변환을 이용해 복소수 계수로 변환하였습니다.

이를 통해 모델의 원래 공간 가중치(w)를 직접 보는 대신, 이를 주파수 영역의 계수(z)로 변환하여 분석하면, 주파수와 은닉 노드 인덱스 사이에서 매우 규칙적인 패턴을 그리며 배치되는 것을 알 수 있습니다.

$z_{pkj} = \sum_{m=0}^{d-1} w_{pmj} e^{imk/d}$

$w_{pmj}$ : 신경망의 hidden layer 에 연결된 가중치 (일반적 DL/ML의 파라미터 Weight)  
$e^{imk/d}$: 푸리에 기저(지표함수) 로 Weight 의 숫자를 특정 주기를 가진 파동 패턴으로 매핑하는 역할  
$z_{pkj}$: 푸리에 기저를 통해 주파수로 변환된 복소수 계수. 이 계수에서 수학적 패턴을 발견함  
$k$: 주파수 (frequency): 패턴의 주기  
$j$: 히든 레이어 노드 인덱스  

#### 차수
특정 주파수(k) 성분을 처리하기 위해 신경망이 할당한 은닉 노드(뉴런)의 개수를 의미합니다. 예를 들어, 주파수 k=1인 구역에 6개의 노드가 활성화되어 있다면 이를 Order-6 해라고 부릅니다.  
(노드가 활성화 되었다 = 해당 노드의 복소수 계수 가 역치값 0.05를 넘었다. $z_{pkj} > 0.05$)

#### Semi-Ring
Semi-ring 구조 적용은 가중치 공간을 숫자 집합이 아닌 "연산 법칙"이 적용 되는 공간으로 인식한다는 의미입니다.
- 링 덧셈(+): 두 신경망의 은닉 노드들을 가로로 이어 붙이는(Concatenate) 연산입니다. 이는 서로 다른 논리를 가진 뉴런들을 하나의 네트워크로 합치는 것과 같습니다.
- 링 곱셈(∗): 은닉 차원을 따라 가중치 간의 크로네커 곱(Kronecker product)을 수행하는 연산입니다.

#### Ring Homomorphism
가중치 공간의 대수적 연산(덧셈, 곱셈) 결과를 함수 값의 계산 결과에서도 그대로 보존하는 것을 의미합니다.

수학적으로 함수 $r(z): \mathcal{Z} \mapsto \mathbb{C}$가 Ring Homomorphism(환 준동형 사상)이라는 것은, 가중치 공간 $\mathcal{Z}$ 에서의 대수적 연산 결과가 함수를 거친 후의 결과에서도 그대로 보존됨을 의미합니다. 구체적으로 다음 세 가지 조건을 만족해야 합니다.
- 항등원 보존: $r(1) = 1$ (가중치 공간의 항등원이 함수값 1로 매핑됨).
- 덧셈 보존: $r(z_1 + z_2) = r(z_1) + r(z_2)$ (두 네트워크를 이어 붙인 것의 함수값은 각 네트워크 함수값의 합과 같음).
- 곱셈 보존:*$r(z_1 * z_2) = r(z_1)r(z_2)$ (두 네트워크의 크로네커 곱의 함수값은 각 함수값의 곱과 같음).
#### Modular Addition
모듈러 덧셈(Modular Addition, $a+b=c\mod d$)은 딥러닝 모델이 단순히 데이터를 암기하여 검색(Retrieval)하는 것인지, 아니면 진정한 추론(Reasoning)을 수행하는 것인지를 파악하기 위한 핵심적인 벤치마크 사례로 연구됩니다.

신경망은 모듈러 덧셈 문제를 해결하기 위해 내부적으로 단순히 숫자 테이블을 외우는 것이 아니라, Fourier basis 라고 불리는 수학적 표현을 스스로 학습한다는 사실이 발견되었습니다.
조금 자세히 설명하면, 내부적으로 푸리에 기저를 학습한다는 것은 수학적 기호(Symbol)를 주기적인 파동 패턴으로 표현한다는 의미(order-4, order-6)이고,이 푸리에 패턴들을 특정한 대수적 규칙(덧셈,곱셈)에 따라 조립하여 최종 가중치를 구성하는 semi-ring 패턴이 발견되었습니다. 

### Structure of Loss Functions
모듈러 연산에서 L2 손실 함수(MSE)는 신경망의 출력값과 실제 정답인 원-핫 인코딩(one-hot encoding) 벡터 사이의 거리를 최소화하는 역할

$\|Output - one\_hot(c)\|_2^2$

이차 활성화 함수($x^2$)를 사용하는 신경망에서 이 복잡한 비선형 L2 손실 함수가 합 전위(Sum Potentials, SPs)들의 합으로 완벽하게 분해될 수 있음을 증명, 따라서, 아래와 같은 loss function으로 치환 될 수 있습니다.

$$\ell_k(z) = -2r_{kkk} + \sum_{k_1, k_2} |r_{k_1k_2k}|^2 + \frac{1}{4} \left| \sum_{p \in \{a,b\}} \sum_{k'} r_{p,k',-k',k} \right|^2 + \frac{1}{4} \sum_{m \neq 0} \left| \sum_{p \in \{a,b\}} \sum_{k'} r_{p,k',m-k',k} \right|^2$$

- $r(z)= \sum_j \prod z_{pkj}$: 합전위. 모든 은닉 노드(j)에 걸쳐 특정 가중치들의 곱(단항식)을 합산한 값
- $r_{k_1k_2k}$: $a$의 주파수 $k_1$, 입력 $b$의 주파수 $k_2$, 그리고 출력 $c$의 주파수 $k$에 대응하는 복소수 가중치들을 모든 노드에서 곱하고 더한 값

이 복잡한 식을 경사하강법을 이용해서 미분해서 푸는 대신에 CoGS는 이 손실 함수의 복잡한 조건들을 대수적인 조립(Composing)을 통해 만족 시킬 수 있습니다.

### Composing Global Optimizers from Partial Ones
Ring Homomorphism 성질을 이용해 복잡한 최적화 문제를 '조립 가능한' 형태로 해석해서 부분적인 제약 조건만 만족하는 '부분 해(Partial solutions)'들을 대수적인 연산(환의 덧셈 및 곱셈)을 통해 결합하여 '전역 최적해(Global optimal solutions)'를 구성할 수 있다고 합니다.

### Exemplar constructed global optimizers
위 "Structure of Loss Functions"의 복잡한 손실함수를 한 번에 미분해서 푸는 대신, CoGs는 수학적인 블록 조립 과정으로 복잡한 신경망의 학습 과정을 설명할 수 있습니다.

#### Order-6 Global Solution ($z_{F6}$)
​가장 대표적인 해로, 주파수별로 3개의 노드를 가진 부품과 2개의 노드를 가진 부품을 곱해 만든 6개의 은닉 노드를 가진 해입니다.
$$
z_{F6} = \frac{1}{\sqrt{6}} \sum_{k=1}^{(d-1)/2} z_{syn}^{(k)} * z_{\nu}^{(k)} * y_k
$$

- $z_{syn}^{(k)}$ (Order-3 Partial Solution): 3개의 은닉 노드를 사용해 손실 함수의 특정 항들(예: $R_c$의 일부)을 0으로 만듭니다.
- $z_{\nu}^{(k)}$ (Order-2 Partial Solution): $z_{syn}$ 이 처리하지 못한 나머지 제약 조건($R_n$ 등)을 해결합니다. 이 둘을 곱($*$)함으로써 모든 조건을 만족하게 됩니다.
- $y_k$ (Unit): 링 구조에서의 단위 원소(Unit)로, 해의 구조를 유지하면서 차원을 맞추는 역할을 합니다.

#### Mixed Order-4/6 Global Solution ($z_{F4/6}$)
​실제 경사 하강법 학습에서 가장 빈번하게 관찰되는 해의 형태로, 대부분의 주파수에서는 더 효율적인 Order-4 해를 사용하고, 특정 주파수에서만 Order-6 해를 섞어 쓰는 형태입니다.  $z_{F6}$는 완벽하지만 노드를 6개나 써야 해서 비용이 비싸기 때문에 AI 가 선택한 전략입니다.

$$ z_{F4/6} = \underbrace{\hat{z}_{F6}^{(k_0)}}_{\text{해결사 (1개)}} + \underbrace{\sum_{k \neq k_0} z_{F4}^{(k)}}_{\text{가성비 부품들 (나머지)}} $$

- $z_{F4}^{(k)}$ (가성비 부품, 노드 4개): 대부분의 주파수($k \neq k_0$)에서 사용하는 해입니다. Order-2 부품 두 개($z_{\nu=i} * z_{\xi}$)를 곱해서 만들어 **노드 4개(Order-4)**만 씁니다. $z_{F6}$보다 노드를 적게 써서 효율적이지만, 수학적으로 아주 미세한 오차(총합 제약 조건 위배)를 남깁니다.
- $\hat{z}_{F6}^{(k_0)}$ (해결사, 노드 6개): 특정 주파수 $k_0$ 딱 하나에 대해서만 비싼 **Order-6** 해를 사용합니다. 단순히 $z_{F6}$를 쓰는 게 아니라, 나머지 $z_{F4}$들이 남긴 미세한 오차들을 전부 계산해서 상쇄시켜 버립니다.

#### Perfect Memorization ($z_M$)
데이터의 규칙을 학습하는 것이 아니라, 모든 데이터 포인트($d \times d$ 테이블)를 단순히 암기해버리는 해입니다.

$$
z_M = d^{-2/3} z_{\alpha} * z_{\beta}
$$

### Gradient Descent solutions matches with construction
연구자들은 신경망의 가중치 공간이 가진 반환(Semi-ring) 구조와 환 준동형 사상(Ring Homomorphism) 성질을 이용하여, 경사 하강법 없이 수학적 연산(링 덧셈 및 곱셈)만으로 최적해를 미리 만들어 보았습니다. 그 후, 우리가 흔히 쓰는 경사 하강법(Gradient Descent)으로 학습시킨 실제 모델의 가중치를 분석해 보았습니다.

1."Structure of Loss Functions" - 경사하강법으로 학습 가능한 일반적인 경우의 손실함수
2."Exemplar constructed global optimizers" - CoGS에서 만든 대수적으로 구성된 최적해 계산법

1을 이용해 학습한 모델과 2를 비교해서 95~98% 일치를 얻어서 CoGS 프레임워크의 예측력과 타당성을 성공적으로 검증되었습니다.

### Conclusion
모델이 푸리엘 기저를 학습해 가중치에서 일정한 패턴을 찾을 수 있고, 나아가 다음 레이어에서 합과 곱으로 형성되는 semi-ring 구조이면서 부분의 연산결과가 합/곱의 연산 이후에도 보존되는 ring homomorphism 성질을 띄는것을 발견할 수 있었습니다. 
이를 이용해 연구자들은 발견한 푸리에 기저 패턴에서 도출한 손실함수를 정의할 수 있는데 이 손실함수는 부분해를 이용해 전체해를 구하는 방식으로 정의할 수 있었습니다.

### Possible Implications
1. 신경망이 학습을 통해 우리가 미처 몰랐던 효율적인 심볼릭 표현을 스스로 학습하고 있을 가능성을 보여줍니다. 
2. 경사 하강법 대신 대수적 결합을 통한 새로운 형태의 학습 알고리즘이 탄생할 수 있습니다.
3. 신경망 내부의 작동 원리를 수학적 원칙에 따라 투명하게 이해할 수 있는 경로를 제시합니다.