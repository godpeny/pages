# CS294/194-196 LLM Agent: 9. Open Source & Science w/ Foundation Models

## Why does access matter?
최근 몇 년간 파운데이션 모델(Foundation Models)의 성능은 급격히 향상되었지만, 아이러니하게도 모델에 대한 접근성(Access)은 그와 대칭적으로 급격히 감소했습니다. 과거에는 논문, 가중치(weights), 코드, 데이터에 모두 접근할 수 있었지만, 현재는 API 접근만 가능한 경우가 많습니다.

접근성이 중요한 이유는 "접근성이 연구의 방향을 결정(Access shapes research)"하기 때문입니다. 역사적으로 접근 가능한 자원은 연구의 흐름을 주도해 왔습니다.
- 1990년대: 인터넷을 통해 디지털 형태의 텍스트에 접근할 수 있게 되면서 통계적 NLP(Statistical NLP) 방법론이 등장했습니다.
- 2010년대 초반: Amazon Mechanical Turk와 같은 크라우드소싱 플랫폼을 통해 ImageNet, SQuAD 같은 대규모 라벨링 데이터셋이 구축되었습니다.
- 2010년대 중반: GPU에 대한 접근성이 높아지면서 딥러닝 방법론이 폭발적으로 성장했습니다.

즉, 어떤 자원에 접근할 수 있느냐에 따라 우리가 풀 수 있는 문제와 연구 방법이 달라집니다.

## Levels of access for foundation models
강의에서는 파운데이션 모델에 대한 접근 수준을 크게 세 가지로 분류하며, 이를 과학자의 역할에 비유하여 설명합니다.
1. API Access (API 접근)
블랙박스 모델의 내부를 볼 수 없고, 프롬프트(입력)를 넣고 반응(출력)을 관찰하여 행동을 측정하는 방식입니다. 이 수준에서도 복잡한 에이전트를 구축하는 등 많은 연구가 가능합니다.
2. Open-Weight Access (오픈 웨이트 접근)
모델 내부의 활성화(activations)를 들여다볼 수 있습니다. 이 수준에서는 모델의 메커니즘을 이해하거나, 증류(distillation) 및 파인튜닝(fine-tuning) 같은 새로운 파생 모델을 만들 수 있습니다. 하지만 여전히 기존 모델의 아키텍처와 설계 내에서만 연구가 가능합니다.
3. Open-Source Access (오픈 소스 접근)
시스템의 모든 부분(데이터, 아키텍처, 학습 과정 등)을 통제하고 수정할 수 있습니다.진정한 오픈 소스는 모든 것에 대해 의문을 제기하고 개선할 수 있는 기회를 제공합니다.

## API Access Model
### Agent architecture
블랙박스 API를 활용하여 구축하는 에이전트의 기본 아키텍처는 다음과 같은 흐름을 가집니다:
1. 지각(Perceive): 에이전트는 현실 세계의 관찰(Observation)을 받아들입니다.
2. 메모리 스트림(Memory Stream): 모든 경험을 기록합니다. 디지털 에이전트이므로 잊어버릴 필요 없이 모든 것을 저장합니다.
3. 검색(Retrieve): 저장된 방대한 기억 중에서 현재 상황에 관련된 기억을 가져옵니다.
4. 행동 결정: 검색된 기억을 바탕으로 세 가지 중 하나를 수행합니다.
- 과거 성찰(Reflect): 과거의 기억을 요약하거나 검증합니다.
- 미래 계획(Plan): 장기적인 방향성을 설정합니다.
- 현재 행동(Act): 도구를 사용하거나 현실 세계에 영향을 미치는 행동을 취합니다.

### Tale of two agents
#### Problem-solving agents
문제 해결 에이전트는 주어진 목표를 달성하기 위해 도구를 사용하고 환경과 상호작용합니다.

##### MLAgentBench
머신러닝 실험 과정을 자동화하는 것입니다. (예: CIFAR-10 모델의 성능 향상시키기).
에이전트는 연구 보조원처럼 행동합니다. 코드를 작성하고(write), 실행하고(execute), 결과 로그를 검사하고(inspect), 다시 코드를 수정하는 과정을 반복합니다.  
ReAct 프레임워크를 기반으로 성찰(Reflection), 계획(Plan), 팩트 체크(Fact Check), 행동(Action)의 단계를 거칩니다.  
Claude 3와 같은 모델이 가장 좋은 성능을 보였으나, 과제에 따라 성능 편차가 큽니다. 이는 모델이 스스로를 개선하는 "자가 개선(Self-improvement)" 연구로 이어질 수 있습니다.

##### CyBench
사이버 보안 챌린지(CTF, Capture the Flag)를 수행하는 것입니다. 서버에 침투하여 취약점을 찾고 'Flag'를 획득해야 합니다. 에이전트는 서버 접근 권한, 로컬 코드, 그리고 Bash 쉘을 사용하여 명령어를 실행할 수 있습니다.

가장 어려운 문제는 인간 팀이 24시간 걸려 해결한 반면, 현재 최고 성능의 모델(Claude 3.5 Sonnet)도 17.5% 정도만 해결할 수 있었습니다.  

정책 입안자에게는 AI의 위험성(Risk)을 정량화하는 도구가 되고, 보안 전문가에게는 방어 목적의 침투 테스트(Penetration testing) 도구가 됩니다.

#### Simulation agents
시뮬레이션 에이전트는 인간의 행동이나 사회를 모사하는 데 초점을 맞춥니다.

##### Generative Agents 1
Smallville'이라는 가상 공간에서 25명의 에이전트가 상호작용하는 시뮬레이션입니다. 에이전트끼리 대화하며 정보(예: 발렌타인 파티 소식)가 퍼져나가는 과정을 시뮬레이션합니다.

- 검색(Retrieval): 모든 기억 중 최신성(Recency), 중요성(Importance), 관련성(Relevance)을 기준으로 필요한 기억을 추출합니다.
- 성찰(Reflection): "냉장고가 비었다" 같은 낮은 수준의 기억들을 추상화하여 "Klaus는 연구에 헌신적이다"와 같은 고차원적인 통찰을 만들어냅니다.

##### Generative Agents 2
가상의 캐릭터가 아닌 실존 인물을 시뮬레이션하는 프로젝트입니다. 사회과학 실험을 위한 '디지털 쌍둥이(Digital Twin)'를 만드는 것이 목표입니다. 1,000명의 다양한 인구 통계학적 배경을 가진 사람들을 대상으로 2시간 동안 심층 인터뷰를 진행하고, 이 인터뷰 내용을 에이전트의 기억(Memory)으로 사용합니다.

실제 참가자와 에이전트에게 동일한 설문(GSS, 성격 검사 등)을 수행하게 하여 비교했습니다. 에이전트는 실제 사람의 응답을 약 85%의 정확도로 모사했습니다. 이는 같은 사람이 2주 뒤에 다시 설문했을 때의 일치도(81%)와 매우 유사한 수준입니다. 이는 사회과학 연구를 위한 강력한 도구가 될 수 있음을 시사합니다.

## Open-weight access Model
정확히는 "Weights 를 널리 이용할 수 있는 이중 용도 파운데이션 모델(Dual-use foundation models with widely available weights)" 여기서 '널리 이용 가능(widely available)'하다고 표현하는 이유는 Llama와 같은 모델들의 라이선스가 완전한 오픈 소스가 아니라 특정 조건(예: 월간 활성 사용자 7억 명 이상은 별도 라이선스 필요)을 포함하기 때문입니다.  
API 모델은 제공자가 모델을 업데이트하거나 더 이상 제공하지 않을 수 있어(deprecated), 과거의 실험 결과를 재현하기 어렵습니다. 반면, 오픈 웨이트 모델은 가중치를 다운로드하여 영구적으로 보관할 수 있으므로 **재현성(Reproducibility)**을 보장합니다.  

오픈 웨이트 모델은 단순히 사용하는 것을 넘어 모델 내부를 들여다볼 수 있기 때문에 해석 가능성(Interpretability), 파인튜닝(Fine-tuning), 증류(Distillation), 모델 병합(Merging), 적대적 공격(Adversarial attacks) 등 다양한 연구를 가능하게 합니다.  

여전히 기존 모델의 아키텍처와 설계도(Blueprint) 안에서만 연구가 이루어진다는 한계가 있습니다.
### Monitor: An AI-Driven observability Interface
오픈 웨이트 접근이 가능할 때 수행할 수 있는 대표적인 연구 사례로 해석 가능성(Interpretability) 도구인 'Monitor'를 소개합니다. 모델 내부를 들여다봄으로써 모델의 이상 행동을 디버깅하거나, 모델이 정답을 알면서도 대답을 거부하는지 등을 파악할 수 있습니다.  
<b> 문제 상황</b>: Llama 3.1 8B 모델에게 "9.8과 9.11 중 무엇이 더 큰가?"라고 물으면 "9.11이 더 크다"라고 잘못된 답을 합니다. API만 있다면 왜 이런 답을 하는지 알 수 없습니다.  

가중치에 접근할 수 있다면 각 뉴런이 어떤 입력에 반응하는지(trigger) 확인할 수 있습니다. 연구진은 각 뉴런이 반응하는 데이터를 분석해 뉴런의 역할을 요약했습니다. 
"9.11"에 반응하는 뉴런은 수학적 숫자가 아니라 "9·11 테러(September 11)"와 같은 날짜와 관련된 뉴런이었습니다. 날짜로 해석한다면 9월 11일이 9월 8일보다 뒤에 있으므로(크므로), 모델이 그런 답변을 한 것임이 밝혀졌습니다.

### Compact Language Modle via Pruning Knowledge distillation
오픈 웨이트 모델을 활용한 또 다른 연구 사례는 모델을 경량화하는 것입니다.  
Nvidia의 연구에서는 150억(15B) 파라미터 모델을 가져와서 레이어를 Pruning 하고 다시 학습시켜 80억(8B), 40억(4B) 모델로 만들었습니다. 이를 통해 성능 저하를 최소화하면서 모델의 크기를 줄이는 데 성공했습니다.

거대 모델의 능력을 작은 모델로 Transfer 할 수 있다는 것을 보여줍니다. 이는 에이전트 시스템에서 특히 중요한데, 에이전트는 내부 루프에서 API를 반복 호출해야 하므로 추론 속도가 빠른 작은 모델이 필요하기 때문입니다.

### Universial and Transferable Adversarial Attack 
오픈 웨이트 모델은 보안 및 적대적 공격(Adversarial Attack) 연구에도 필수적입니다.  
모델을 Jailbreak 시켜 악의적인 행동을 하게 만드는 프롬프트 접미사(Suffix)를 찾기 위해, 모델의 기울기(Gradient) 정보를 이용하여 최적화를 수행합니다. 이는 가중치에 접근할 수 있어야만 가능합니다.  

또 발견한 점은 오픈 웨이트 모델(예: Llama 3)을 대상으로 최적화한 공격 프롬프트를 GPT-4와 같은 폐쇄형 API 모델에 입력해도 탈옥이 성공한다는 것입니다.

### Independece Test for Language Models
허깅페이스(Hugging Face) 등에는 수많은 모델이 올라오지만, 이 모델들이 실제로 독자적으로 학습된 것인지 기존 모델(예: Llama 2)을 파인튜닝한 것인지 불명확한 경우가 많습니다. 이를 해결하기 위해 "두 모델의 가중치가 주어졌을 때, 이들이 독립적으로 학습되었는가?"를 판별하는 테스트를 제안합니다.  

#### Idea 1
- 방법: 두 모델 가중치 간의 유사도(예: 코사인 유사도)를 계산합니다.
- 문제점: 계산된 유사도 값(예: 0.1)이 높은 것인지 낮은 것인지 판단할 기준이 없습니다. 통계적 보증(Statistical guarantees)이 불가능합니다.

#### Idea 2
- 방법 (사고 실험): 모델의 가중치를 만든 훈련 과정을 안다고 가정하고, 동일한 과정으로 랜덤 시드만 다르게 하여 여러 번 재학습시킵니다. 이렇게 만든 '복제 모델'들과 오리지널 모델 사이의 유사도 분포를 구하고, 복제 모델과의 유사도가 이 분포에서 어디에 위치하는지(P-value) 확인합니다.
- 문제점: 모델이 어떻게 학습되었는지 알 수 없을뿐더러, 테스트를 위해 거대 모델을 수없이 재학습하는 것은 비용적으로 불가능합니다.

#### Idea 3
- 방법: 모델을 실제로 재학습하는 대신, 은닉 유닛(Hidden units)의 순서를 Permute 하여 마치 다른 초기화에서 학습된 것 같은 '가상의 모델'을 생성합니다.
- 작동 원리: 모델1 의 가중치 행렬을 치환한 perm(모델1 가중치)와 모델2 가중치 사 이의 유사도를 계산하여, 실제 두 모델의 가중치 사이의 유사도가 우연에 의한 것인지 검정합니다 (P-value 계산)
- 결과: 독립적으로 학습된 모델끼리는 P-value가 고르게 분포한 반면 파인튜닝된 모델(종속된 모델)끼리는 P-value가 0에 가깝게 나옵니다.
- 적용 사례: 이 테스트를 통해 'Miku' 모델이 Mistral의 유출본임을, 'StripedHyena'가 Mistral에서 파생되었음을, 그리고 Llama 3.2 3B 모델이 Llama 3.1 8B의 특정 레이어에서 파생되었음을 수학적으로 입증했습니다.

## Open-Source access Model
### What exactly is open-source?
'오픈 소스'라는 용어는 단순히 "공개되었다"는 의미를 넘어 역사적 맥락과 철학을 가집니다.
- 기원: 1950년대 MIT의 해커 윤리(Hacker ethic)와 학계의 열린 과학(Open science) 전통에서 유래했습니다.
- 핵심 가치: 창의성, 탐구, 투명성, 협업, 그리고 권위에 대한 저항을 중시합니다.
- 역사적 이정표:
  - 1983년: 리처드 스톨만(Richard Stallman)의 GNU 프로젝트 시작.
  - 1991년: 리누스 토르발스(Linus Torvalds)의 리눅스(Linux) 시작.
  - 1998년: 오픈 소스 이니셔티브(OSI)가 설립되어 '오픈 소스'라는 용어를 정의했습니다.

### Open-Source AI Definition
OSI는 현재 AI 시대에 맞는 '오픈 소스 AI'의 정의를 정립 중이며, 이는 소프트웨어의 4대 자유(사용, 연구, 수정, 공유의 자유)를 AI에 적용하는 것을 목표로 합니다. 진정한 오픈 소스 AI가 되기 위해서는 다음 세 가지가 제공되어야 합니다.

1. 파라미터(Parameters): 가중치(Weights)가 제한 없이 제공되어야 합니다.
2. 코드(Code): 단순히 추론 코드가 아니라, 학습(Training) 코드와 데이터 처리 및 필터링 코드까지 모두 포함되어야 합니다. 데이터가 어떻게 처리되었는지 모르면 모델의 행동을 온전히 이해할 수 없기 때문입니다.
3. 데이터 정보(Data Information): 학습에 사용된 데이터에 대한 충분히 상세한 정보가 필요합니다.

#### Data information, not data
여기서 중요한 점은 '데이터 자체(Data)'가 아니라 '데이터 정보(Data Information)'를 요구한다는 것입니다. 이유는 대부분의 파운데이션 모델은 웹 데이터(Web data)로 학습되는데, 이 데이터의 저작권 문제(Copyright)가 복잡하기 때문입니다. 모델 개발자는 웹 데이터의 소유자가 아니므로, 이를 재배포할 법적 권한(License)이 없습니다.  
따라서 원본 데이터를 직접 공개하는 대신, 숙련된 사람이 동등한 시스템을 구축할 수 있을 정도로 상세한 데이터 설명(Description)과 처리 방식을 공개하는 것을 기준으로 삼습니다.

### DoReMi
언어 모델 학습 시 위키피디아, 책, 웹, 코드 등 다양한 데이터 소스를 어떤 비율(Mixture)로 섞을지 결정하는 것은 보통 직관(Vibes)에 의존하거나 수동으로 튜닝해야 했습니다.
DoReMis는 분포적으로 견고한 최적화(Distributionally Robust Optimization, DRO)를 사용하여 데이터 도메인 간의 가중치를 자동으로 최적화하는 알고리즘입니다.  
그 결과 수동으로 튜닝한 가중치 대비 2.6배 더 빠른 학습 속도로 동일한 성능에 도달했습니다. 이는 처음부터 학습 과정을 통제할 수 있는 오픈소스 아래서만 가능한 연구입니다.

### Sophia
기존의 Adam 옵티마이저는 널리 쓰이지만 계산 비용이 높거나 효율성 한계가 있었습니다.2차 정보(Second-order information)인 대각 헤시안(Diagonal Hessian)과 클리핑(Clipping)을 활용한 새로운 옵티마이저인 Sophia를 제안했습니다.  
그 결과 Adam 대비 2배의 속도 향상을 이루었습니다. 이 역시 학습 알고리즘 자체를 수정할 수 있는 오픈 소스 환경에서만 가능합니다.

### BackPack Language Model
트랜스포머(Transformer) 모델은 거대한 단일 블록 같아서 내부를 해석하거나 제어하기 어렵습니다. 반면 BackPack LM은 해석 가능성과 제어를 위해 설계된 새로운 아키텍처입니다. 단어를 문맥 의존적인 '센스 벡터(Sense vectors)'의 합으로 표현합니다.  
BackPack LM은 정밀한 모델 수정(Precise model editing)이 가능합니다. 예를 들어, 모델이 "MacBook은 HP가 만든 것이다"라고 믿도록 특정 벡터만 수정하여 개입할 수 있습니다.

### Would the results hold if we scaled up?
이러한 연구들은 작은 규모에서 수행되었기에, "과연 거대 모델(Scale up)에서도 통할 것인가?"라는 질문을 받게 됩니다. 결국 이는 "컴퓨팅 파워(Compute)를 어디서 구할 것인가?"의 문제로 귀결됩니다. 이에 대해 세 가지 트랙(Track)을 제시합니다.

#### Track 1:Scaling Laws (스케일링 법칙 활용)
작은 규모의 실험 결과를 큰 규모로 추정(Extrapolate)할 수 있는 스케일링 법칙을 연구하는 것입니다. 현재의 스케일링 법칙은 주로 "더 크게 만드는 것"에 초점이 맞춰져 있지만, 반대로 "작은 규모에서 한 실험이 큰 규모에서도 유효할지"를 검증하는 방향으로 연구가 필요합니다. 이를 통해 적은 자원으로도 의미 있는 연구를 할 수 있습니다.

#### Track 2
전 세계에 흩어져 있는 유휴(Idle) GPU들을 모아서 하나의 거대한 슈퍼컴퓨터처럼 활용하자는 것입니다.

##### The Problem
데이터센터 내부의 GPU들은 초고속 인터커넥트(Interconnect)로 연결되어 있지만, 인터넷을 통해 전 세계 GPU를 연결하면 대역폭이 100배 이상 느리고 지연(Latency)이 발생합니다.

##### Decentralized Training of Foundation Models in Heterogenous Environment
느린 인터커넥트 환경에서도 영리한 스케줄링과 시스템 최적화를 통해 10억(1B) 파라미터 모델을 학습시켰을 때, 데이터센터 대비 약 2배 정도만 느린 속도로 학습이 가능함을 보였습니다. 이는 생각보다 탈중앙화 학습이 실현 가능함을 시사합니다.

##### DiLoCo
구글 딥마인드의 연구로, 통신 빈도를 획기적으로 줄여서(Low-Communication) 서로 연결이 좋지 않은(poorly connected) 기기들의 집합에서도 언어 모델을 효과적으로 학습시키는 알고리즘입니다.  
최근 'Prime Intellect' 같은 곳에서는 이를 통해 100억(10B) 파라미터 모델을 탈중앙화 방식으로 학습하는 시도를 하고 있습니다.

#### Track 3
학계와 오픈 소스 연구를 위한 국가적 차원의 인프라를 구축하는 것입니다. 미국의 NAIRR (National AI Research Resource) 파일럿 프로그램이 대표적입니다.  
물리학의 LHC(대형 강입자 충돌기)나 천문학의 제임스 웹 우주 망원경(James Webb Telescope)처럼, 상업적 이익이 아닌 순수 학문적 탐구를 위해 거대 인프라를 공공 자금으로 지원해야 합니다. 기업(OpenAI, Anthropic)이 하지 않을, 학계만이 할 수 있는 질문을 탐구하기 위해서입니다.

