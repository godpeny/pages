# Reflextion
Reflexion은 메인 에이전트(Actor)를 고정한 채 에이전트의 경험을 바탕으로 메모리 모듈을 동적으로 업데이트한다는 점에서 T2(에이전트 감독형 도구 적응, Agent-Supervised Tool Adaptation) 프레임워크의 대표적인 사례로 분류.

Reflexion의 핵심은 에이전트의 가중치를 수정하지 않고 '언어적 피드백'을 통해 외부 메모리를 최적화하여 성능을 높이는 구조

에이전트가 과업을 수행하는 과정은 세 가지 모델의 협력을 통해 이루어집니다.
<img src="./Reflextion2.png" alt="Reflextion" width="500"/>  

- 궤적 생성 (Actor): 에이전트(Actor)는 주어진 환경에서 행동을 취하며 자신의 활동 기록인 과업 수행 경로(Trajectory)를 생성합니다.
- 결과 평가 (Evaluator): 평가자 모델이 이 궤적을 검토하여 성공 여부나 보상 점수(r)를 매깁니다. 만약 실패(예: 이진 보상 0)로 판정되면 피드백 루프가 활성화됩니다.
- 언어적 반성 (Self-Reflection): 자가 반성 모델은 실패한 궤적과 평가 신호를 분석하여, "어느 지점에서 실수가 발생했는지"와 "다음에는 어떻게 개선해야 하는지"를 자연어 텍스트로 요약합니다. 이를 Reflexion 논문에서는 수학적 그래디언트에 대비되는 '의미론적 그래디언트(Semantic Gradient)'라고 부릅니다.

이 반성 텍스트는 장기 메모리(Long-term memory) 에 저장된 다음 시도(Trial)에서 에이전트는 이전에 저장된 반성 내용들을 자신의 프롬프트 문맥(Context)으로 주입받습니다. 이를 통해 에이전트는 동일한 실수를 반복하지 않고, 이전의 실패 경험을 바탕으로 더 정교한 계획을 세울 수 있게 됩니다.

즉 이 프레임워크에서 메모리 모듈은 에이전트의 내부 파라미터가 아니라, 에이전트가 읽고 쓸 수 있는 '외부 도구(T)'로 간주됩니다. 그리고  Reflexion을 수행하는 동안 메인 LLM(Actor)의 내부 가중치는 전혀 변하지 않습니다. 대신, 에이전트가 내놓은 출력(실패 기록)을 신호로 삼아 메모리라는 도구의 내용이 업데이트(최적화)됩니다. T2