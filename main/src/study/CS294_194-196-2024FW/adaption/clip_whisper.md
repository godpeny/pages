# CLIP - 시각 모델
인터넷의 자연어 설명(텍스트)을 직접 시각 모델의 학습 신호로 사용하는 방법론

CLIP(Contrastive Language-Image Pre-training)**은 에이전트와 독립적으로 훈련되어 다양한 시스템에 즉시 삽입될 수 있는 T1(에이전트 무관 도구 적응, Agent-Agnostic Tool Adaptation) 프레임워크의 핵심적인 도구 사례로 분류됩니다.

## T1 특징
- CLIP은 인터넷에서 수집한 4억 개의 (이미지, 텍스트) 쌍을 사용하여 대규모 대조 학습(Contrastive Learning)을 통해 처음부터 끝까지 독립적으로 훈련되었습니다. 
- 별도의 추가 훈련 없이 바로 호출하여 사용할 수 있는 '플러그 앤 플레이(Plug-and-play)' 지원.
- 도구 적응은 에이전트의 내부 파라미터를 건드리지 않고 외부 생태계를 풍부하게 함으로써 시스템의 유연성을 높입니다.

# Whisper 
웹에서 수집한 대규모의 '전사 데이터(오디오와 텍스트 쌍)'를 통해 인간 수준의 정확도와 강건함을 가진 음성 처리 시스템을 만드는 연구

Whisper는 인터넷에서 수집한 680,000시간의 다국어 및 다중 작업(multitask) 감독 데이터를 사용하여 처음부터 끝까지 독립적으로 학습하였습니다. Whisper를 동결된(frozen) API 형태로 사용합니다. 에이전트가 오디오 입력을 전달하면 Whisper는 음성 인식(ASR), 번역, 언어 식별 결과를 반환하며, 이 과정에서 에이전트나 Whisper 모델 중 어느 쪽도 서로를 위해 파라미터를 수정하지 않습니다.

